{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoC Conversational Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmDgxzuW-dwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvwS_VplT3fk",
        "colab_type": "code",
        "outputId": "ddd4de1d-cbdf-4e5e-ce24-3eab305b50f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qys-CEEVi3JO",
        "colab_type": "code",
        "outputId": "6f600299-f3c5-457d-c23a-0caa2a3b1e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#reading data from the movie_conversations.txt file\n",
        "movie_conversations = open('movie_conversations.txt','r')\n",
        "conversations = []    #contains all the conversations as list elements ( list within a list )\n",
        "for i in movie_conversations.readlines():\n",
        "  a = i.split('+++$+++')\n",
        "  b = a[3].split(', ') \n",
        "  conv= []\n",
        "  for j in b:\n",
        "    conv.append(re.sub(r'[^\\w]', ' ',j).replace(\" \",\"\"))\n",
        "  conversations.append(conv)\n",
        "conversations[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['L194', 'L195', 'L196', 'L197'],\n",
              " ['L198', 'L199'],\n",
              " ['L200', 'L201', 'L202', 'L203'],\n",
              " ['L204', 'L205', 'L206'],\n",
              " ['L207', 'L208']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOxom6VHu7zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading data from the movie_lines.txt\n",
        "movie_lines = open('movie_lines.txt','rb')\n",
        "code_vs_dialogue_list = dict()\n",
        "for line in movie_lines:\n",
        "  i = line.decode(encoding='utf-8',errors='ignore')\n",
        "  a = i.split('+++$+++')\n",
        "  conv_code = a[0].replace(\" \",\"\")\n",
        "  dialogue = a[4].replace(\"\\n\",\"\").strip()\n",
        "  code_vs_dialogue_list.update({conv_code:dialogue})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2w0KKfy4pdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dialogue(x):\n",
        "    return code_vs_dialogue_list[x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeMr859C9e7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "for i in conversations:\n",
        "  for k in range(len(i)-1):\n",
        "    questions.append(find_dialogue(i[k]))\n",
        "    answers.append(find_dialogue(i[k+1])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmr9gf5fgvS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_with_tags = []\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( '<START> ' + answers[i] + ' <END>' )\n",
        "    else:\n",
        "        questions.pop( i )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G583aXfNg9lK",
        "colab_type": "code",
        "outputId": "791c9372-3a7f-4e55-b3c4-c8d18ffe320b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers_with_tags)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd0QjHcXNojH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upper_limit = 20\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers_with_tags )\n",
        "final_tokens_questions = []\n",
        "final_tokens_answers = []\n",
        "for i in range(len(tokenized_questions)):\n",
        "  if len(tokenized_questions[i])<=upper_limit:\n",
        "    final_tokens_questions.append(tokenized_questions[i])\n",
        "    final_tokens_answers.append(tokenized_answers[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5O7EG8Pg93Y",
        "colab_type": "code",
        "outputId": "a78eed66-84a7-4354-c44d-c67a86e3b6da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#encoder_input_data_preprocessing\n",
        "#maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
        "#padded_questions = tf.keras.preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
        "padded_questions = tf.keras.preprocessing.sequence.pad_sequences( final_tokens_questions , maxlen=upper_limit , padding='post' )\n",
        "encoder_input_data = np.array( padded_questions )\n",
        "print( encoder_input_data.shape )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(195670, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg7d6IlhhI6V",
        "colab_type": "code",
        "outputId": "0aff5eb0-44d9-44a8-c707-6958744866f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# decoder_input_data\n",
        "#tokenized_answers = tokenizer.texts_to_sequences( answers_with_tags )\n",
        "#maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = tf.keras.preprocessing.sequence.pad_sequences( final_tokens_answers , maxlen=upper_limit , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(195670, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJQ5yrMCThU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(final_tokens_answers)) :\n",
        "    final_tokens_answers[i] = final_tokens_answers[i][1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi6gnpNshJOx",
        "colab_type": "code",
        "outputId": "df18930a-4cd1-49b8-d554-a9353c4ea298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# decoder_output_data\n",
        "#tokenized_answers = tokenizer.texts_to_sequences( answers_with_tags )\n",
        "#maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_decout_answers = tf.keras.preprocessing.sequence.pad_sequences( final_tokens_answers , maxlen= upper_limit, padding='post' )\n",
        "padded_decout_answers.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(195670, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4vjQ_puSlZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters\n",
        "embedding_dim = 256\n",
        "lstm_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExrjUalV-jmR",
        "colab_type": "code",
        "outputId": "b6bc8c6a-e37b-4b9b-bbe5-6e0611e3f5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#training cell with training model created.\n",
        "inference_encoder_inputs = tf.keras.layers.Input(shape=(None,)) #'''batch_size=batch_size''' )\n",
        "embedded_encoder_inputs = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inference_encoder_inputs)\n",
        "encoder_output,state_h,state_c = tf.keras.layers.LSTM(lstm_units, return_state=True,return_sequences=True)(embedded_encoder_inputs) # '''input_shape=(batch_size,input_length,embedding_dim)'''\n",
        "encoder_states = [state_h,state_c]\n",
        "\n",
        "inference_decoder_inputs = tf.keras.layers.Input(shape=(None,)) #''',batch_size=batch_size''')\n",
        "embedded_decoder_inputs = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inference_decoder_inputs)\n",
        "decoder_lstm_layer = tf.keras.layers.LSTM(lstm_units, return_state = True, return_sequences=True) #''',input_shape=(batch_size,input_length,embedding_dim)'''\n",
        "decoder_output,_,_ = decoder_lstm_layer(embedded_decoder_inputs, initial_state = encoder_states)\n",
        "decoder_output = tf.reshape(decoder_output,(-1,decoder_output.shape[2]))\n",
        "dense_layer = tf.keras.layers.Dense(vocab_size, activation=tf.keras.activations.softmax)\n",
        "decoder_output = dense_layer(decoder_output)\n",
        "\n",
        "training_model = tf.keras.models.Model([inference_encoder_inputs,inference_decoder_inputs],decoder_output)\n",
        "training_model.compile(optimizer = tf.keras.optimizers.RMSprop(), loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "training_model.summary()\n",
        "\n",
        "# Some important points - \n",
        "# raw inputs and outputs are used to define the model ( without embedding ). \n",
        "# NEVER overwrite tensors, because the LSTM layer requires brand new embedded layer\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    14138112    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    14138112    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, None, 1024), 5246976     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 1024), 5246976     embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape (TensorFlow [(None, 1024)]       0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 55227)        56607675    tf_op_layer_Reshape[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 95,377,851\n",
            "Trainable params: 95,377,851\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fb3Rv_k7fsa",
        "colab_type": "code",
        "outputId": "5acc32d1-c938-4fa6-d190-0d55e284570b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_model.load_weights(\"training_2/cp.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc66c251dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzi4vTXsNS1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "checkpoint_path = \"training_2/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmDAIisID7TF",
        "colab_type": "code",
        "outputId": "65489c63-37a7-4a16-9e2e-04415b027ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "training_model.fit([encoder_input_data,decoder_input_data],padded_decout_answers,batch_size=170, epochs = 2, callbacks=[cp_callback])\n",
        "training_model.save('trainingmodel_2.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1151/1151 [==============================] - ETA: 0s - loss: 1.6898\n",
            "Epoch 00001: saving model to training_2/cp.ckpt\n",
            "1151/1151 [==============================] - 2050s 2s/step - loss: 1.6898\n",
            "Epoch 2/2\n",
            "1151/1151 [==============================] - ETA: 0s - loss: 1.6276\n",
            "Epoch 00002: saving model to training_2/cp.ckpt\n",
            "1151/1151 [==============================] - 2051s 2s/step - loss: 1.6276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8zMg6amEACi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_inference_models():\n",
        "  inference_encoder_model = tf.keras.models.Model(inference_encoder_inputs,encoder_states)\n",
        "\n",
        "  decoder_state_input_h = tf.keras.layers.Input(shape=(lstm_units, ))\n",
        "  decoder_state_input_c = tf.keras.layers.Input(shape=(lstm_units, ))\n",
        "  decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
        "\n",
        "  decoder_output,state_h,state_c = decoder_lstm_layer(embedded_decoder_inputs, initial_state = decoder_states_inputs)\n",
        "  decoder_states = [state_h,state_c]\n",
        "  decoder_output = dense_layer(decoder_output)\n",
        "\n",
        "  inference_decoder_model = tf.keras.models.Model([inference_decoder_inputs]+decoder_states_inputs, [decoder_output]+decoder_states)\n",
        "\n",
        "  return inference_encoder_model,inference_decoder_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UINCywCgeijR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return tf.keras.preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=upper_limit , padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8y_02Hwzz7w",
        "colab_type": "code",
        "outputId": "d012dd43-a395-4ca9-fbfb-39d48ec1a452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > upper_limit:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter question : hi\n",
            " hi end\n",
            "Enter question : how are you\n",
            " fine end\n",
            "Enter question : good to know\n",
            " i don't know what you're talking about end\n",
            "Enter question : you are smart\n",
            " yes end\n",
            "Enter question : okay then bye\n",
            " bye end\n",
            "Enter question : see you \n",
            " yeah end\n",
            "Enter question : good night\n",
            " good night end\n",
            "Enter question : good morning\n",
            " good morning end\n",
            "Enter question : good evening \n",
            " good night end\n",
            "Enter question : you are dumb \n",
            " no end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obx7BGtK0Ktg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}